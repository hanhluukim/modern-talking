\documentclass[english,handout]{mlutalk}

\title{Stance Classification for Key-Point Analysis}
% \title{%
%   Modern Talking: Key-Point Analysis \\
%   using Modern Natural Language Processing
% }
\subtitle{Natural Language Processing, Summer Semester 2021}
\author{Max Henze \and Hanh Luu \and Jan Heinrich Reimer}
\institute{Martin Luther University Halle-Wittenberg}
\date{\today}
\titlegraphic{\includegraphics[width=3cm]{figures/mlu-halle}}

\addbibresource{../literature/literature.bib}

\usepackage{tikz}
\usepackage{listings}
\usepackage{xspace}
\usepackage{biblatex}
\usepackage{tabularx}
\usepackage{booktabs}

\newcommand{\Bert}{\textsc{Bert}\xspace}
\newcommand{\ArgKP}{\mbox{ArgKP}\xspace}
\newcommand{\ArgQ}{\mbox{IBM-ArgQ-Rank-30kArgs}\xspace}
\newcommand{\BiLSTM}{\mbox{BiLSTM}\xspace}
\newcommand{\BertBase}{\textsc{Bert}-Base\xspace}
\newcommand{\BertLarge}{\textsc{Bert}-Large\xspace}
\newcommand{\TF}{\mbox{TF}\xspace}
\newcommand{\TFIDF}{\mbox{TF/IDF}\xspace}

\begin{document}

\titleframe

\begin{frame}{Stance Classification}

  \begin{block}{Motivation}
    Is an argument \(A\) pro, con or something else to a topic \(T\)?
  \end{block}

  \begin{example}
    \footnotesize\centering\vspace*{1ex}
    \begin{tabular}{llc}
      \toprule
      \(T\) & Nuclear energy & \\
      \midrule
      \(A_1\) & Nuclear energy is a cheap alternative to fossil fuels. & pro \\
      \(A_2\) & Failures in nuclear power plants resolve in catastrophic events. & con \\
      \(A_3\) & The CDU decided on a nuclear phase-out in 2021/22. & non-arg \\
      \(A_4\) & I don't like nuclear energy because my mom also doesn't like it. & discuss \\
      \bottomrule
    \end{tabular}
  \end{example}

  \begin{block}{Problem}
    \begin{itemize}
      \item What makes an argument pro or con?
      \item Can it always be decided?
      \item Can an argument be classified pro by looking at other pro-like arguments?
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}{Stance Classification}
  \framesubtitle{For Key Point Matching}
  
  \begin{itemize}
    \item Key points have a stance like arguments
    \item It's unlikely that key points and argument of different stance match
    \item Key points generation / summarization benefits from stance information~\cite{SanchanAB2017}
    \item Stance can be used to balance generated key points~\cite{ChowandaSSS2017}
  \end{itemize}

  \begin{example}
    \footnotesize\centering\vspace*{1ex}
    \begin{tabular}{llc}
      \toprule
      \(A_1\) & Nuclear energy is a cheap alternative to fossil fuels. & pro \\
      \(K_1\) & Nuclear energy is cheap. & pro \\
      \midrule
      \(A_2\) & Failures in nuclear power plants resolve in catastrophic events. & con \\
      \(K_2\) & Nuclear power plants pose a security risk. & con \\
      \bottomrule
    \end{tabular}
  \end{example}

\end{frame}

\begin{frame}{Stance Classification}

  \begin{block}{Framework}
    \begin{equation*}
      f: (D,T) \rightarrow S \in \mathbb{S}
    \end{equation*}  
    \begin{itemize}
      \item Known stances~\(\mathbb{S}\) often includes pro and con, \\ can include task-specific stances
      \item Document~\(D\) can be argument or key point (or any sentence)
      \item Topic~\(T\) may be unknown
    \end{itemize}
  \end{block}

  \begin{block}{Machine Learning Pipeline}
    \begin{enumerate}
        \item Extract features from documents and topic
        \item Build machine learning model for \(f\)
        \item Train model
        \item Test model
      \end{enumerate}
  \end{block}

\end{frame}

\begin{frame}[allowframebreaks]{Fake News Challenge Stance Detection~\cite{HanselowskiSSCC2018}}

  \begin{block}{Challenge}
    \begin{itemize}
      \item Stance $S$\,: argee, disagree, discuss, or unrelated
      \item Predict stance for a document's headline
    \end{itemize}
  \end{block}

  \begin{block}{Contributions}
    \begin{itemize}
      \item Reproduction and analysis of the challenge's top-performing systems
      \begin{itemize}
        \item Tablos
        \item Athene
        \item UCL
      \end{itemize}
      \item What features/architectures improve performance most?
      \item Evaluation metrics
    \end{itemize}
  \end{block}
  
  \framebreak

  \begin{block}{Architectures and Features}
    \footnotesize\centering\vspace*{1.5ex}
    \begin{tabularx}{\linewidth}{lp{0.25\linewidth}X}
      \toprule
      & \textbf{Model} & \textbf{Features} \\
      \midrule
      \textbf{Talos} & weighted model~(CNN, GDB~tree) & Word2Vec, \TFIDF, sentiment \\
      \textbf{Athene} & MLP (softmax, 6~hidden layers) & BoW, unigrams, similarity, topic models, latent semantic \\
      \textbf{UCL} & MLP (1~hidden layer) & unigram term frequency, \TFIDF, similarity \\
      \textbf{baseline} & gradient boosting & co-occurrence, refuting word count, polarity word count \\
      \bottomrule
    \end{tabularx}
  \end{block}

  \begin{block}{Helpful Features}
    \begin{itemize}
      \item Co-occurrence of words and \(n\)-grams
      \item Bag of words, bag of character 3-grams
      % \item Topic model features (negative matrix factorization)
      \item Latent semantic indexing / latent Dirichlet allocation
      \item GloVe embeddings
    \end{itemize}
  \end{block}
    
  \framebreak

  \begin{block}{Difficulties}
    \begin{itemize}
      \item lexical overlap between headline and document
      \item synonyms treated as unrelated
      \item few lexical indicators for disagree stance
      \item semantic relations / content understanding
    \end{itemize}
  \end{block}
  
  \begin{block}{Evaluation metrics}
    \begin{itemize}
      \item Hiearchical evaluation in the task
      \begin{itemize}
        % TODO Define labels or use descriptive names.
        \item $\{ REL=\{AGR, DSG, DSG\}, UNR\}$: 0.25 points
        \item $\{AGR, DSG, DSG\}$: 0.75 points
        \item Imbalanced labels: UNR dominates
      \end{itemize}
      \item Suggested metric: macro-averaged $F_1$ score 
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[allowframebreaks]{\Bert Same Side Stance Classification~\cite{OllingerDSBS2020}}
  
  \begin{block}{Problem: Same Side Stance Classification}
    \begin{itemize}
      \item Are both arguments pro or both con?
      \item Simplification of general stance classification
      \item Idea: use language model for semantics
    \end{itemize}
  \end{block}

  \begin{example}
    \footnotesize\centering\vspace*{1ex}
    \begin{tabular}{llc}
      \toprule
      \(A_1\) & Nuclear energy is a cheap alternative to fossil fuels. & pro \\
      \(A_2\) & Failures in nuclear power plants resolve in catastrophic events. & con \\
      \(A_3\) & Nuclear waste is a burden on many future generations. & con \\
      \bottomrule
    \end{tabular}

    \normalfont\raggedright\vspace*{1.5ex}
    Arguments \(A_1\) and \(A_2\) have different stances, \(A_2\) and \(A_3\) have the same.
  \end{example}
  
  \framebreak
  
  \begin{block}{Approach}
    \begin{itemize}
      \item \Bert architecture to infer semantic similarity
      \item Fine-tune \BertBase and \BertLarge models for 3~epochs
      \item Sequences limited to 512 tokens (position embedding length limit)
    \end{itemize}
  \end{block}

  \begin{block}{Results}
    \begin{itemize}
      \item \Bert classifier outperforms SVM baseline
      \item \BertLarge slightly better than \BertBase
      \item Longer sequences (i.e., less truncated) do not perform much better %\\ (most sequences are short anyways)
      \item Classifier with \Bert can learn from partially truncated sentences
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[allowframebreaks]{Cross-topic Argument Mining~\cite{StabMSRG2018}}
  Topic of this paper:
  \begin{itemize}
    \item To search for arguments, relevant to a certain topic 
    \item Given a heterogeneous document collection
  \end{itemize}
  Key Achievements found in this paper:
  \begin{itemize}
    \item Non, supporting and opposing styled annotation scheme
    \item New corpus including 27,520 sentences for 8 controversial topics
    \item Contextual BiLSTM leads to better generalization to unknown topics
    \item Multi-task learning setup further improved this approach 
    % Topic information during training improves prediction?
    % Problems?
  \end{itemize}
  
  \framebreak

  How did they achieve this ?\\ % Try to use blocks \begin{block}{Title} ... \end{block}
  $\rightarrow$ two major approaches for identifying arguments
  \begin{itemize}
    \item Integrating topic information
    \begin{itemize}
      \item Outer-attention BiLSTM 
      \begin{itemize}
        \item Model learning how to weight input words given the topic combined with BiLSTM
      \end{itemize}
      \item Custom contextual BiLSTM
      \begin{itemize}
        \item Adding topic information as additional input term 
      \end{itemize}
    \end{itemize}
    \item Leveraging additional data (two additional corpora)
    \begin{itemize}
      \item Transfer learning
      \begin{itemize}
        \item train model twice with auxiliary corpus first 
      \end{itemize}
      \item Multi-task learning
      \begin{itemize}
        \item using RNN for separate learning and combination afterwards
      \end{itemize}
    \end{itemize}
  \end{itemize}
  % Maybe "flatten" the key points a bit
\end{frame}

\begin{frame}{Comparison}
  \begin{itemize}
    \item What stances to consider in key point matching?
    \item Fake News Challenge can give hints on feature selection
    \item Same side stance classification is necessary condition to key point matching
    \item Simpler architectures work reasonably well
    \item Pretrain classifier on additional datasets
  \end{itemize}
  \thankyou
\end{frame}

\appendix
\section{\appendixname}

\bibliographyframe

\end{document}
