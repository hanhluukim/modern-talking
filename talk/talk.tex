\documentclass[english,handout]{mlutalk}

\title{Stance Classification for Key-Point Analysis}
% \title{%
%   Modern Talking: Key-Point Analysis \\
%   using Modern Natural Language Processing
% }
\subtitle{Natural Language Processing, Summer Semester 2021}
\author{Max Henze \and Hanh Luu \and Jan Heinrich Reimer}
\institute{Martin Luther University Halle-Wittenberg}
\date{\today}
\titlegraphic{\includegraphics[width=3cm]{figures/mlu-halle}}

\addbibresource{../literature/literature.bib}

\usepackage{tikz}
\usepackage{listings}
\usepackage{xspace}
\usepackage{biblatex}

\newcommand{\Bert}{\textsc{Bert}\xspace}
\newcommand{\ArgKP}{\mbox{ArgKP}\xspace}
\newcommand{\ArgQ}{\mbox{IBM-ArgQ-Rank-30kArgs}\xspace}
\newcommand{\BiLSTM}{\mbox{BiLSTM}\xspace}
\newcommand{\BertBase}{\textsc{Bert}-Base\xspace}
\newcommand{\BertLarge}{\textsc{Bert}-Large\xspace}
\newcommand{\TF}{\mbox{TF}\xspace}
\newcommand{\TFIDF}{\mbox{TF/IDF}\xspace}

\begin{document}

\titleframe

\begin{frame}{Stance Classification} 
  \begin{block}{Motivation}
    \begin{itemize}
      \item Given an argument A and a topic T: Can A be classified as pro, con or something else to T.
    \end{itemize}
  \end{block}
  \begin{block}{Problem}
    \begin{itemize}
      \item What makes an argument pro, con or else ? 
      \item Can an argument be classified pro by looking at other pro-like arguments ?
    \end{itemize}
  \end{block}
  \begin{example}
    \begin{tabular}{l|l}
      T & \multicolumn{1}{c}{Nuclear Energy}\\
      \hline
      $A_1$ & "Nuclear energy is a cheap alternative to fossil fuels."\\
      $A_2$ & "Failures in nuclear power plants resolve in catastrophic events."\\
      $A_3$ & "The CDU decided on a nuclear phase-out in 2021/22."\\
      $A_4$ & "I don't like nuclear energy because my mom also doesn't like it."
    \end{tabular}
  \end{example}
\end{frame}

\begin{frame}{Approaches} % Hanh
  % Mathematical framework
  
  \begin{block}{Features Extraction from training dataset}
    \begin{itemize}
        \item extract Features from documents and headlines  
    \end{itemize}
  \end{block}
  \begin{block}{Machine Learning Methods}
    \begin{itemize}
        \item machine learning methods to learn $f$
    \end{itemize}
  \end{block}
  
  \begin{block}{Input}
  \begin{itemize}
      \item  Document $d$ and a headling $h$
      \item  $s$ is a stance, $\mathbb{S}$ known stances set
  \end{itemize}
  \end{block}
  \begin{block}{Output}
    
    \[f: (d,h) \rightarrow s \in \mathbb{S}\]
   
  \end{block}
\end{frame}

\begin{frame}[allowframebreaks]{Fake News Challenge Stance Detection~\cite{HanselowskiSSCC2018}}
  %\begin{example}
    % Sample document for different stances
    % Sample headline
  %\end{example}
  \begin{block}{Challenge}
    \begin{itemize}
      \item Stance $s$: argee, disgree, discuss, unrelated
      \item Predict a label stance for a document with regard to a headline
    \end{itemize}
  \end{block} % Try to use blocks \begin{block}{Title} ... \end{block}
  \begin{block}{Topic of this paper}
      \begin{itemize}
        \item Reproduction and Analysis for the top-performing systems of the challenge: \textit{Tablos, Athene, UCL}
        \item Which features and architectures used help improving performance?
        \item Evaluation metrices
      \end{itemize}
  \end{block}
  
  \framebreak
  \begin{block}{Three best approaches}
    \begin{table}[]
        \caption{architectures and features used by the three best approaches}
        \label{tab:comparison_approaches}
        \begin{tabular}{|l|l|l|}
        \hline
                 & Modell            & Features                                                                                      \\ \hline
        Talos &
          \begin{tabular}[c]{@{}l@{}}weighted model\\ (CNN and GDB Tree)\end{tabular} &
          \begin{tabular}[c]{@{}l@{}}CNN: word2vec\\ GDBT: TFIDF, Sentiment, word2vec\end{tabular} \\ \hline
        Athene &
          \begin{tabular}[c]{@{}l@{}}MLP (Softmax, \\ 6 h-layers)\end{tabular} &
          \begin{tabular}[c]{@{}l@{}}BoW, Unigrams\\ Similarity (Nouns, Verbs)\\ Topic Models, Latent Semantic\end{tabular} \\ \hline
        UCL      & MLP (1h-layers)   & \begin{tabular}[c]{@{}l@{}}TF of unigrams\\ TFIDF\\ Similarity\end{tabular}                   \\ \hline
        Baseline & Gradient-Boosting & \begin{tabular}[c]{@{}l@{}}cooccurrence $(h, d)$\\ counts refuting and polarity words\end{tabular} \\ \hline
        \end{tabular}
    \end{table}
    \end{block}
    
    \framebreak
    \begin{block}{Features Analysis}
      \begin{itemize}
          \item Fails in the following cases:
            \begin{itemize}
                \item lexical overlap between headline and document (classified as related, but unrelated)
                \item document-headline pair contains synonyms, not same tokens (classified as unrelated)
                \item if \textit{reports, said, allegedly}, then stance: DSC
                \item few lexical indicators available for DSG class
                \item semantic relations, complex negation, understanding of content
            \end{itemize}
          \item Helpful features:
          \begin{itemize}
              \item Cooccurrence of words and n-grams
              \item Bag of Words, Bag-of-character 3-grams
              \item Topic model features based on negative matrix factorization
              \item Latent Semantic Indexing
              \item Latent Dirichlet Allocation
              \item DSG class improved with semantic based on wordembedding GloVe
          \end{itemize}
      \end{itemize}
    \end{block}
    
    \framebreak
  
    \begin{block}{Evaluation metrics}
      \begin{itemize}
          \item Hiearchical evaluation in this task
            \begin{itemize}
                \item $\{ REL=\{AGR, DSG, DSG\}, UNR\}$: 0.25 points
                \item $\{AGR, DSG, DSG\}$: 0.75 points
                \item Imbalanced Problem (class UNR)
            \end{itemize}
          \item Suggested metric: macro-averaged $F_1$ Score 
      \end{itemize}
    \end{block}
\end{frame}
% Try to continue slide with frame breaks

\begin{frame}[allowframebreaks]{\Bert Same Side Stance Classification~\cite{OllingerDSBS2020}}
  
  \begin{block}{Problem}
    Same side  Stance Classification
    \begin{itemize}
      \item Are both arguments pro or both con?
      \item Simplification of general stance classification
      \item 
    \end{itemize}
  \end{block}

  \begin{example}
    % Add topic for sample
    % Example structure: topic and sample arguments (claim, premise)
    \begin{align}
      p_1 &= \text{"Renewable energy can soon replace fossil/nuclear energy"} \\
      p_2 &= \text{"Nuclear energy is a cheap alternative to fossil fuels"} \\
      p_3 &= \text{"The danger from radioactive contamination should be avoided"}
    \end{align}
    Sentences \(p_1\) and \(p_2\) have different stances, \(p_1\) and \(p_3\) have the same.
  \end{example}
  
  \framebreak
  
  \begin{block}{Approach}
    \begin{itemize}
      \item \Bert architecture for classification
      \item Fine-tune \BertBase and \BertLarge models for 3~epochs
      \item Sequences limited to 512 tokens (due to position embedding length limit)
    \end{itemize}
  \end{block}

  \begin{block}{Results}
    \begin{itemize}
      \item \Bert classifier outperforms SVM baseline
      \item \BertLarge slightly better than \BertBase
      \item Longer sequences (i.e., less truncated) do not perform much better \\ (most sequences are short anyways)
      \item Classifier with \Bert can learn from partially truncated sentences
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}{Cross-topic Argument Mining~\cite{StabMSRG2018}}
  Topic of this paper:
  \begin{itemize}
    \item To search for arguments, relevant to a certain topic 
    \item Given a heterogeneous document collection
  \end{itemize}
  Key Achievements found in this paper:
  \begin{itemize}
    \item Non, supporting and opposing styled annotation scheme
    \item New corpus including 27,520 sentences for 8 controversial topics
    \item Contextual BiLSTM leads to better generalization to unknown topics
    \item Multi-task learning setup further improved this approach 
    % Topic information during training improves prediction?
    % Problems?
  \end{itemize}
\end{frame}
% Try to continue slide with frame breaks

\begin{frame}{Cross-topic Argument Mining~\cite{StabMSRG2018}}
  How did they achieve this ?\\ % Try to use blocks \begin{block}{Title} ... \end{block}
  $\rightarrow$ two major approaches for identifying arguments
  \begin{itemize}
    \item Integrating topic information
    \begin{itemize}
      \item Outer-attention BiLSTM 
      \begin{itemize}
        \item Model learning how to weight input words given the topic combined with BiLSTM
      \end{itemize}
      \item Custom contextual BiLSTM
      \begin{itemize}
        \item Adding topic information as additional input term 
      \end{itemize}
    \end{itemize}
    \item Leveraging additional data (two additional corpora)
    \begin{itemize}
      \item Transfer learning
      \begin{itemize}
        \item train model twice with auxiliary corpus first 
      \end{itemize}
      \item Multi-task learning
      \begin{itemize}
        \item using RNN for separate learning and combination afterwards
      \end{itemize}
    \end{itemize}
  \end{itemize}
  % Maybe "flatten" the key points a bit
\end{frame}

%\begin{frame}{Comparison} % Hanh
%  TODO: compare approaches
%\end{frame}

\begin{frame}{Stance Classification in Key Point Matching} % Heini
  TODO: how can we use stance classification for matching key points?
  \thankyou
\end{frame}

% \begin{frame}{Conclusion}
%   TODO: what to do next?
% \end{frame}

% \begin{frame}{Future Work}
%   TODO: do we need future work?
%   \thankyou
% \end{frame}

\appendix
\section{\appendixname}

\bibliographyframe

\end{document}
