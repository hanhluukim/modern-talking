\section{Related Work}\label{related-work}

% \todo{Shorten subsections. We probably don't need subsection headers in this section.}

% \subsection{Key Point Analysis}



Similar tasks to key point analysis include clustering arguments~\cite{reimers2019classification,ajjour2019modeling}, detecting similar arguments in a pairwise fashion~\cite{misra:2016} and matching arguments to generic-arguments~\cite{naderi:2017}. Using points to summarize arguments were approached by~\citet{egan2016summarising} on online discussion. Points were extracted by using the verbs and their syntactic arguments and are then clustered together to deliver a summary of the discussion. 

Key point analysis is the task of matching a given argument with one or more pre-defined key points~\cite{Bar-HaimEFKLS2020}. To develop models for the task, \citet{Bar-HaimEFKLS2020} introduced a dataset (\ArgKP) which contains 24 093~argument key point pairs on 28~topics. Each argument and key point is labeled manually as \texttt{match} or \texttt{no-match}. The authors experimented with several unsupervised and supervised approaches to perform the task in a cross-topic experimental setting. BERT~\cite{DevlinCLT2019} performed the best in their experiments by reaching an F1 score of $0.68$.

In a later work, \citet{Bar-HaimKEFLS2020} develop a summarization approach for online discussions that
uses key point analysis. The summrization approach takes as input a set of comments on a given topic and returns a set of representative key points from them, each key point with the count of matched comments. In its essence, the summarization approach uses a matching model that gives a score for a given comment and key point or a couple of key points. For matching models, \citet{Bar-HaimKEFLS2020} compare different variants of BERT~\cite{DevlinCLT2019}. Among the tested models,
ALBERT \cite{lan2019albert} performed the best with an F1 score $0.809$, but RoBERTa~\cite{LiuOGDJCLLZS2019} were chosen for key point extraction at the end, which is 6 times faster than ALBERT and still achieves an F1 score of $0.773$. 

\Bert stands for Bidirectional Encoder Representations from Transformers and is an open-source bidirectional language representation model published by Google~\cite{DevlinCLT2019}. 
%\Bert is based on the transformer architecture, however, \Bert only uses the encoder in multi-layers. 
\Bert is pre-trained over unlabeled text to learn a language representation and can be fine-tuned on downstream tasks. During pre-training, \Bert is trained on two unsupervised tasks: Maked Language Model and Next Structure Prediction. \Bert achieved state-of-the-art results on eleven Natural Language Processing tasks \cite{WangSMHLB2018}, for example, Textual Entailment which is similar to key point nalysis.

\Roberta is an improved variant of \Bert that is introduced by Facebook in 2019~\cite{LiuOGDJCLLZS2019}. \citet{LiuOGDJCLLZS2019} tweaked \Bert by using a larger training data size of 160GB of uncompressed text, more compute power, larger batch-training size, and optimized hyperparameters. Unlike in the BERT model, \Roberta is pre-trained with sequences of with full-length sequences of 512~tokens. In comparison to \Bert, pre-training tasks for \Roberta include only Maked Language Model while applying different masks in each training epoch (dynamic masking). Results in the GLUE leaderboard have shown that \Roberta outperforms BERT on all 9 GLUE tasks in the single-task setting and 4 out of 9 tasks in the ensembles setting~\cite{WangSMHLB2018,LiuOGDJCLLZS2019}.


% \subsection{Argument Clustering}
% Argument Clustering is a mighty tool that enables algorithms to assign multiple arguments, which adress a similar
% key message to a given topic. \citet{reimers2019classification} make use of 
% "contextualize word embeddings to classify and cluster topic-dependet arguments". Having performed argument
% classification they then compute similar and dissimilar pairs of arguments. Two approaches one with clustering
% and one without are being used. 
% Clustering arguments is achieved by usage of agglomerative hierarchical clustering \cite{day1984efficient}. 
% Without clustering a fine-tuned BERT-base-uncased model reached a F1 mean score for similar and dissimilar
% arguments of $0.7401$. 
% Agglomerative hierarchical clustering being a strict partitioning algorithm, results for clustering perform
% worse by up to 7.64pp (Bert-large F1 mean score: $0.7135$). Hence they conclude that "strict partitioning 
% clustering methods introduce a new source of errors".
% Another approach proposed by \citet{ajjour2019modeling} revolves around clustering 
% arguments into so called frames which are "a set of arguments that focus on the same aspect". 
% Thereby framing \cite{entman1993framing} only a specific information to present to the listeners and convince 
% them of your stance.
% They propose that an argument consists of two crucial parts. The topic and the frame.
% Hence their approch splits into three steps: First, all arguments are clustered into $m$ topics. Second,
% topical features are extracted from all arguments and therefore from its cluster. Third, the arguments are 
% reclustered into $k$ non-overlapping frames. By utilizing k-means \cite{hartigan1979ak} for clustering
% and Term Frequency-Inverse Document Frequency (TF-IDF) for topic removal they achieved a F1 score of 
% $0.28$.

% \subsection{Pretrained Language Models}



