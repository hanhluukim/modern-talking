\section{Related Work}\label{related-work}

% \todo{Shorten subsections. We probably don't need subsection headers in this section.}

% \subsection{Key Point Analysis}
Key point analysis is the task of matching a given argument with one or more pre-defined key points~\cite{Bar-HaimEFKLS2020}. To develop models for the task, \citet{Bar-HaimEFKLS2020} introduced a dataset \ArgKP which contains 24 093~argument key point pairs on 28~topics. Each argument and key point is labeled manually as \texttt{match} or \texttt{no-match}. The authors experimented with several unsupervised and supervised approaches to perform the task in a cross-topic experimental setting. BERT~\cite{DevlinCLT2019} performed the best in their experiments by reaching an F1 score of $0.68$.

In a later work, \citet{Bar-HaimKEFLS2020} develop a summarization approach for online discussions that
uses key point analysis. The summrization approach takes as input a set of comments on a given topic and returns a set of representative key points from them, each key point with the count of matched comments. In its essence, the summarization approach uses a matching model that gives a score for a given comment and key point or a couple of key points. For matching models, \citet{Bar-HaimKEFLS2020} compare different variants of BERT~\cite{DevlinCLT2019}. Among the tested models,
ALBERT \cite{lan2019albert} performed the best with an F1 score $0.809$, but RoBERTa~\cite{LiuOGDJCLLZS2019} were chosen for key point extraction at the end, which is 6 times faster than ALBERT and still achieves an F1 score of $0.773$. 

\Bert stands for Bidirectional Encoder Representations from Transformers and is an open-source bidirectional language representation model published by Google~\cite{DevlinCLT2019}. 
%\Bert is based on the transformer architecture, however, \Bert only uses the encoder in multi-layers. 
\Bert is pre-trained over unlabeled text to learn a language representation and can be fine-tuned on downstream tasks. During pre-training, BERT is trained on two unsupervised tasks: Masked Language Model and Next Structure Prediction. \Bert achieved state-of-the-art results on eleven natural language processing tasks \cite{WangSMHLB2018}, for example, Textual Entailment which is close to Key Point Analysis.

\Roberta (robustly optimized \Bert approach= is introduced by Facebook in 2019~\cite{LiuOGDJCLLZS2019}. 
\Roberta is an improved variant of BERT with improved training methods, such as a larger training data size of 160GB of uncompressed text, more compute power, larger batch-training size, and optimization of BERTâ€™s multiple hyperparameters. 
In the \Roberta model, the peak learning rate, number of warmup steps are tuned for each setting and another value for one Adam optimizer parameter is applied to improve the stability during training. 
Unlike in the BERT model, \Roberta is pre-trained with sequences of at most 512 tokens and with full-length sequences. 
\Roberta removed Next Structure Prediction and uses dynamic masking. 
Results in the GLUE leaderboard have shown that \Roberta outperforms BERT on all 9 GLUE tasks in the single-task setting and 4 out of 9 tasks in the ensembles setting~\cite{WangSMHLB2018,LiuOGDJCLLZS2019}.

\citet{egan2016summarising} propose a summarising for informal arguments such as they
occure in online political debates. By extracting verbs and their syntactic arguments they retrieve points which can make key content accessible. By grouping these points they propose to create discussion summaries.

% \subsection{Argument Clustering}
% Argument Clustering is a mighty tool that enables algorithms to assign multiple arguments, which adress a similar
% key message to a given topic. \citet{reimers2019classification} make use of 
% "contextualize word embeddings to classify and cluster topic-dependet arguments". Having performed argument
% classification they then compute similar and dissimilar pairs of arguments. Two approaches one with clustering
% and one without are being used. 
% Clustering arguments is achieved by usage of agglomerative hierarchical clustering \cite{day1984efficient}. 
% Without clustering a fine-tuned BERT-base-uncased model reached a F1 mean score for similar and dissimilar
% arguments of $0.7401$. 
% Agglomerative hierarchical clustering being a strict partitioning algorithm, results for clustering perform
% worse by up to 7.64pp (Bert-large F1 mean score: $0.7135$). Hence they conclude that "strict partitioning 
% clustering methods introduce a new source of errors".
% Another approach proposed by \citet{ajjour2019modeling} revolves around clustering 
% arguments into so called frames which are "a set of arguments that focus on the same aspect". 
% Thereby framing \cite{entman1993framing} only a specific information to present to the listeners and convince 
% them of your stance.
% They propose that an argument consists of two crucial parts. The topic and the frame.
% Hence their approch splits into three steps: First, all arguments are clustered into $m$ topics. Second,
% topical features are extracted from all arguments and therefore from its cluster. Third, the arguments are 
% reclustered into $k$ non-overlapping frames. By utilizing k-means \cite{hartigan1979ak} for clustering
% and Term Frequency-Inverse Document Frequency (TF-IDF) for topic removal they achieved a F1 score of 
% $0.28$.

% \subsection{Pretrained Language Models}



