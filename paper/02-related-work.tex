\section{Related Work}\label{related-work}

\subsection{Key Point Analysis}
Given the first track of the shared task analyzing key points is crucial. In their work \cite{Bar-HaimEFKLS2020} 
Bar-Haim et al. propose an approach for summarising large argument collections to small sets of key points. Thus
covering a sufficient amount of all arguments. They show that domain experts can very quickly
create pro and con key points, which are able to "capture the gist" of the arguments on the given topic. All this
without being exposed to the arguments themself. Furthermore they develop the large-scale dataset ArgKP
which is the foundation of this shared task. 
In a later work \cite{Bar-HaimKEFLS2020} Bar-Haim et al. construct an automatic method for key point 
extraction which can compete with key points created by human domain experts. The method consists of two aspects. 
Assuming that the key points can be found among the given comments 
they first select short, high quality comments as key point candidates an then select the candidate with the highest
data coverage. Using the HuggingFace transformer framework they fine-tune four different models from which 
ALBERT \cite{lan2019albert} has the best F1 score with $0.809$ but RoBERTa \cite{LiuOGDJCLLZS2019} (F1 score of 
$0.773$) is chosen for key point extraction since it has a 6 times faster inference time. 
In the paper \cite{egan2016summarising}, Egan et al. propose a summarising for informal arguments such as they
occure in online political debates. By extracting verbs and their syntactic arguments they retrieve points which
can make key content accessible. By grouping these points they propose to create discussion summaries.

\subsection{Argument Clustering}
Argument Clustering is a mighty tool that enables algorithms to assign multiple arguments, which adress a similar
key message to a given topic. In the paper \cite{reimers2019classification}, Reimers et al. make use of 
"contextualize word embeddings to classify and cluster topic-dependet arguments". Having performed argument
classification they then compute similar and dissimilar pairs of arguments. Two approaches one with clustering
and one without are being used. 
Clustering arguments is achieved by usage of agglomerative hierarchical clustering \cite{day1984efficient}. 
Without clustering a fine-tuned BERT-base-uncased model reached a F1 mean score for similar and dissimilar
arguments of $0.7401$. 
Agglomerative hierarchical clustering being a strict partitioning algorithm, results for clustering perform
worse by up to 7.64pp (Bert-large F1 mean score: $0.7135$). Hence they conclude that "strict partitioning 
clustering methods introduce a new source of errors".
Another approach proposed in a paper \cite{ajjour2019modeling} by Ajjour et al. revolves around clustering 
arguments into so called frames which are "a set of arguments that focus on the same aspect". 
Thereby framing \cite{entman1993framing} only a specific information to present to the listeners and convince 
them of your stance.
They propose that an argument consists of two crucial parts. The topic and the frame.
Hence their approch splits into three steps: First, all arguments are clustered into $m$ topics. Second,
topical features are extracted from all arguments and therefore from its cluster. Third, the arguments are 
reclustered into $k$ non-overlapping frames. By utilizing k-means \cite{hartigan1979ak} for clustering
and \textit{Term Frequency-Inverse Document Frequency} (TF-IDF) for topic removal they achieved a F1 score of 
$0.28$.

\todo{Third paper: Argument Invention from First Principles (probably not crucial)}

\subsection{Stance Classification}
By knowing the stance of an argument it becomes nearly effortless for a human to classify it as pro or con 
to a given topic. 
