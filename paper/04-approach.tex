\section{Approach}\label{approach}

To match key points to arguments, we propose two different approaches.
First, we shortly discuss a simple yet effective baseline measuring term overlap between key points and arguments.
Second, to improve upon the simple baseline, we introduce a fine-tuning approach using \Bert and \Roberta pretrained language models~\cite{DevlinCLT2019,LiuOGDJCLLZS2019}. We use both language models in standard configuration with only minor changes highlighted below.

\subsection{Term Overlap Baseline}

We start with a very simple baseline. Therefore choosing a Term Overlap baseline with preprocessed terms. 
Generally it can be assumed, that key-points are summarizing ideas of all associated arguments. We therefore came up with the idea
that certain key-words, contained in a lot of arguments, are also very likely to be present in the associated key-point. This makes 
sense with our intuition, because rather than using completly new words for summarization of arguments, a human would 
rather reuse certain important words, which have been already found in the arguments.\\
\todo{Maybe this example fits better into the data section?}
\todo{Make example a figure in a floating environment.}
For example, the following argument exists in the \ArgKP dataset:\\
\textit{People reach their limit when it comes to 
their quality of life and should be able to end their {\color{blue} suffering}. This can be done with little 
or no {\color{blue} suffering} by {\color{orange} assistance} and the person is able to say good bye.}\\ 

In relation to this the following key-point can be found:\\
\textit{{\color{orange} Assisted} suicide reduces {\color{blue} suffering}.}\\

It can already be seen, that an overlap with the words \textit{suffering} exists. 
We can further increse this overlap by performing simple preprocessing steps.\\
First of all, we utilize stop word removal for reducing the noise within all arguments. Initially this can be seen 
counter productive, because less words means less overlap and therefore worse performance. But at second glance this 
makes a lot of sense. A lot of arguments and key-points contain unnecessary words like \textit{the, and, as etc.}.
Removing these gives us purer sentences and results in less confusion with the term-overlap algorithm. Furthermore the 
redundancy of language makes it possible to contain key-aspects in sentences, even with out these unnecessary stop words.\\
Secondly, Stemming reduces terms to their corresponding stems and thus achieves a better generalization, 
when comparing terms. For example, the word: \textit{weakness} will be stemmed to \textit{weak} using the Porter-Stemmer 
\cite{Porter1980}. Thus creating an overlap between those words and increasing the possibility that an argmunt containing
\textit{weakness} will be associated to a key-pont containing \textit{weak}.\\
Thirdly, we increase the generalization of our term-overlap algorithm even further by creating lists of synonyms and 
antonyms and testing if checked words can be replaced with candidates from these to increase overlap.\\
For the actual similarity computation of given arguments and key-points we use the Jaccard similarity coefficient 
\cite{Jaccard1902}. Meaning a higher proportion of terms that appear in an argument as well as in a key-point will 
classify this key-point to be more likely to match.

\subsection{Language Model Fine-tuning}

To improve upon the simple term overlap baseline, we fine-tune \Bert and \Roberta language models for classifying argument key-point matches~\cite{DevlinCLT2019,LiuOGDJCLLZS2019}.
While \Bert is pretrained on a very large document corpus~(16GB of raw data), \Roberta is pretrained on an even larger corpus~(160GB).
Thus \Roberta models can be fine-tuned to higher end task performance~\cite{LiuOGDJCLLZS2019}.
We tokenize both the arguments and the key-points with \Bert's default WordPiece tokenizer and the resulting sequences are trimmed to 512~tokens for both models.
We then fine-tune the \BertBase and \RobertaBase variants in the standard sentence-pair regression setting using the Simple Transformers library.\footnote{\url{https://simpletransformers.ai/}}
For classification, we interpret the regression output value as the probability of an argument matching a key-point, that is, the training labels are always 0~or~1, depending on whether the corresponding pair in the training set matches or not.
Both model variants contain 12~hidden layers with a hidden size of~768 and 12~attention heads.
We train each of the two models for one single epoch at a learning rate of~\( \eta = 2 \cdot 10^{-5} \).
We use an AdamW optimizer with~\( \beta = (0.9, 0.999) \) and zero weight decay.
The optimizer is warmed up for a ratio of~0.06 and we fine-tune both models on binary cross-entropy loss.
We explore different ways of handling argument key-point pairs in the training set with missing ground-truth label, i.e., either removing those pairs, assuming a match~(1), or no match~(0), but find that neither of the last assumptions lead to improved scores on the validation set.
Thus, for the submitted model, we consider only training pairs that have an associated ground-truth label.

