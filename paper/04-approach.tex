\section{Approach}\label{approach}

To match key points to arguments, we propose two different approaches.
First, we shortly discuss a simple yet effective baseline measuring term overlap between key points and arguments.
Second, to improve upon the simple baseline, we introduce a fine-tuning approach using \Bert and \Roberta pretrained language models~\cite{DevlinCLT2019,LiuOGDJCLLZS2019}. We use both language models in standard configuration with only minor changes highlighted below.

\subsection{Term Overlap Baseline}
To be able to compare more sophisticated matchers, we first propose a very simple term overlap baseline using preprocessed terms from each argument and key point. 
In general, key points summarize ideas of their matched arguments.
Our intuition therefore is, that certain words or terms from an argument are also likely to be present in its matched key points.
Rather than using completly new words for summarization of arguments, a human would tend to reuse most important words from the argument.
% \todo{Do they really? Maybe cite a source.}
% \todo{Maybe this example fits better into the data section?}
\begin{figure}
    \begin{tabularx}{\linewidth}{@{}lX@{}}
        Arg. & \texttt{People reach their limit when it comes to their quality of life and should be able to end their {\color{blue} suffering}. This can be done with little or no {\color{blue} suffering} by {\color{orange} assistance} and the person is able to say good bye.} \\
        KP & \texttt{\textcolor{orange}{Assisted} suicide reduces \textcolor{blue}{suffering}.}
    \end{tabularx}
    \caption{Example argument key point pair from the ArgKP dataset~\cite{Bar-HaimEFKLS2020} where terms occur in both the argument and the key point.}
    \label{example-overlap}
\end{figure}
For example, the argument and key point shown in Figure~\ref{example-overlap} both contain the term \texttt{suffering}.

We can further increase the term overlap between arguments and key points by preprocessing their terms:
First, we remove stop words for reducing noise within all arguments.
Initially this can seem counter-productive, because with less words the highest possible overlap would also decrease and therefore could lead to worse performance.
However, a lot of arguments and key points contain unnecessary words like \texttt{the}, \texttt{and}, \texttt{as} etc.
Removing these results in sentences that contain more specific information and thus lead to less confusion with the term-overlap matcher.
Furthermore, the redundancy of language makes it possible to contain key aspects in sentences, even without these mostly meaningless stop words.
As a second preprocessing step, we apply reduce terms to their corresponding stems by applying stemming using the Snowball stemmer~\cite{Porter1980}. 
We expect the term overlap matcher to be able to generalize more when comparing terms.
For example, the word \texttt{weakness} will be stemmed to \texttt{weak} with the Snowball stemmer. 
Consequently, stemming creates an overlap between different forms of the same word and, for instance, increases the probability that an argument containing \texttt{weakness} be associated to a key point containing \texttt{weak}.
Last, we increase generalization for term overlap even further by supplementing the set of terms with synonyms and antonyms. This step should also increase the chance of overlapping terms.

We now compute similarity between an argument and a key point similar to the Jaccard coefficient~\cite{Jaccard1902}:
Let~\(\text{terms}_a\) be the set of terms from an argument~\(a\) and \(\text{terms}_k\)~the set of terms from a key point~\(k\).
We calculate the set of overlapping terms like this:
\begin{equation}
    \text{overlap}_{a,k} = \{ t : t \in \text{terms}_a \land t \in \text{terms}_k \}
\end{equation}
The term overlap matcher returns matching scores based on the overlap size weighted against the minimum size of either the argument or the key point:
\begin{equation}
    \text{score}_{a,k} = \frac{ |\text{overlap}_{a,k}| }{ \min\{ |\text{terms}_a|, |\text{terms}_k| \} }
\end{equation}
That is, pairs with a higher proportion of terms that appear both in the argument as well as in the key point are classified with a higher matching score.

\subsection{Language Model Fine-tuning}

To improve upon the simple term overlap baseline, we fine-tune \Bert and \Roberta language models for classifying argument key point matches~\cite{DevlinCLT2019,LiuOGDJCLLZS2019}.
While \Bert is pretrained on a very large document corpus~(16GB of raw data), \Roberta is pretrained on an even larger corpus~(160GB).
Thus \Roberta models can be fine-tuned to higher end task performance~\cite{LiuOGDJCLLZS2019}.
We tokenize both the arguments and the key points with \Bert's default WordPiece tokenizer and the resulting sequences are trimmed to 512~tokens for both models.
We then fine-tune the \BertBase and \RobertaBase variants in the standard sentence-pair regression setting using the Simple Transformers library.\footnote{\url{https://simpletransformers.ai/}}
For classification, we interpret the regression output value as the probability of an argument matching a key point.
That is, the training labels are always 0~or~1, depending on whether the corresponding pair in the training set matches or not.
Both model variants contain 12~hidden layers with a hidden size of~768 and 12~attention heads.
We train each of the two models for one single epoch at a learning rate of~\( \eta = 2 \cdot 10^{-5} \).
We use an AdamW optimizer with~\( \beta = (0.9, 0.999) \) and zero weight decay.
The optimizer is warmed up for a ratio of~0.06 and we fine-tune both models on binary cross-entropy loss.
We explore different ways of handling argument key point pairs in the training set with missing ground-truth label, i.e., either removing those pairs, assuming a match~(1), or no match~(0), but find that neither of the last assumptions lead to improved scores on the validation set.
Thus, for the submitted model, we consider only training pairs that have an associated ground-truth label.
We don't restrict the model's output to the interval~\([0,1]\)---like we did for the baseline---, as the shared task did not mention constraints on the score value that should be returned by a matcher.
