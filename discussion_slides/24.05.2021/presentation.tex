\documentclass[english,handout]{mlutalk}

\title{Ideas for Approaches}
% \title{%
%   Modern Talking: Key-Point Analysis \\
%   using Modern Natural Language Processing
% }
\subtitle{Natural Language Processing, Summer Semester 2021}
\author{Max Henze \and Hanh Luu \and Jan Heinrich Reimer}
\institute{Martin Luther University Halle-Wittenberg}
\date{\today}
\titlegraphic{\includegraphics[width=3cm]{figures/mlu-halle}}

\addbibresource{../../../literature/literature.bib}

\usepackage{tikz}
\usepackage{listings}
\usepackage{xspace}
\usepackage{biblatex}
\usepackage{tabularx}
\usepackage{booktabs}

\newcommand{\Bert}{\textsc{Bert}\xspace}
\newcommand{\ArgKP}{\mbox{ArgKP}\xspace}
\newcommand{\ArgQ}{\mbox{IBM-ArgQ-Rank-30kArgs}\xspace}
\newcommand{\BiLSTM}{\mbox{BiLSTM}\xspace}
\newcommand{\BertBase}{\textsc{Bert}-Base\xspace}
\newcommand{\BertLarge}{\textsc{Bert}-Large\xspace}
\newcommand{\TF}{\mbox{TF}\xspace}
\newcommand{\TFIDF}{\mbox{TF/IDF}\xspace}

\begin{document}

\titleframe


\begin{frame}{Language Model Approach}
  
  \begin{itemize}
    \item Using BERT we can create the following input structure\\
      \begin{itemize}
        \item \textit{[CLS]} argument \textit{[SEP]} Key Point
      \end{itemize}
    \item Hence learning contextual relations between words
  \end{itemize}

  \begin{tabular}{ll}
    \toprule
      pro & con \\
    \midrule
      often used as baseline & computationally intensive \\
      often good results but not too good & might need to use DistilBERT\\
    \bottomrule
  \end{tabular}

\end{frame}

\begin{frame}{Language Model Approach cont.}
  
    \begin{itemize}
      \item \cite{StabMSRG2018} showed that integrating topic information "has a strong impact on argument prediction"
      \item Therefore maybe use BICLSTM and integrate topic information only on i and c gates
    \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Logistic Regression and Ensemble Voting}

\begin{block}{All approaches with Stemming Snowball}
  \begin{tabular}{lll}
    \toprule
      name & params & mAP (relaxed) \\
    \midrule
      regression-bow & C=14, CountVectorizer & 0.70 \\
      regression-bow-pos & C=14, CountVectorizer, POS & 0.66\\
      regression-tfidf & C=1, TFIDFVectorizer & 0.55 \\
      ensemble-bow & LG, SVC, 0.55, 0.45 & 0.635 \\
      ensemble-bow & LG, SVC, 0.45, 0.55 & 0.647 \\
      ensemble-bow-pos & LG, SVC, 0.55, 0.45 & 0.696 \\
      ensemble-bow-pos & LG, SVC, 0.45, 0.55 & 0.713 \\
      svc-bow-pos & SVC, CountVectorizer, POS &   0.74 \\
      svc-bow & SVC, CountVectorizer & 0.70\\
    \bottomrule
  \end{tabular}
\end{block}
\framebreak
\begin{block}{Further}
\begin{itemize}
    \item features reducing: cutting words by certain length, ex: homeschooling - homesch (get max 6 characters)
    \item instead of stemming, using lemmatization
    \item other solvers for logistic regression
    \item using doc2vec embeddings for features
\end{itemize}
\end{block}
\end{frame}



\appendix
\section{\appendixname}

\bibliographyframe

\end{document}
